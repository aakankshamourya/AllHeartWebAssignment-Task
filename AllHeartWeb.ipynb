{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Scrapping Social Media Links**\n",
        "Beautiful Soup is a library that makes it easy to scrape information from web pages. It sits atop an HTML or XML parser, providing Pythonic idioms for iterating, searching, and modifying the parse tree.\n"
      ],
      "metadata": {
        "id": "vzt3acNB7Fq7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "def scrape_social_links(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    social_links = []\n",
        "    for link in soup.find_all('a'):\n",
        "        href = link.get('href')\n",
        "        if href and any(platform in href for platform in ['facebook', 'twitter', 'linkedin']):\n",
        "            social_links.append(href)\n",
        "    return social_links\n",
        "\n",
        "# Usage\n",
        "url = \"https://allheartweb.com/\"\n",
        "social_links = scrape_social_links(url)\n",
        "print(f\"Scraped social links: {social_links}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PDzeamDR7Cin",
        "outputId": "f0361e97-b200-4fcd-8fcc-4ebfcb77cc33"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraped social links: ['https://www.facebook.com/AllHeartWeb/', 'https://twitter.com/allheartweb', 'https://www.linkedin.com/company/allheartweb/']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PtR514yb8M2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Scrapping Meta Titles From Website**\n"
      ],
      "metadata": {
        "id": "nOtldoBQ9wBl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing the modules\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# target url\n",
        "url = 'https://www.javatpoint.com/'\n",
        "\n",
        "# making requests instance\n",
        "reqs = requests.get(url)\n",
        "\n",
        "# using the BeautifulSoup module\n",
        "soup = BeautifulSoup(reqs.text, 'html.parser')\n",
        "\n",
        "# displaying the title\n",
        "print(\"Title of the website is : \")\n",
        "for title in soup.find_all('title'):\n",
        "\tprint(title.get_text())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZNIUHey98BS",
        "outputId": "b113258e-284c-4989-9c63-837368d21705"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Title of the website is : \n",
            "Tutorials List - Javatpoint\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Meta Description"
      ],
      "metadata": {
        "id": "3RdRQ-sX-Tlf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing the modules\n",
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# target url\n",
        "url = 'https://www.geeksforgeeks.org/'\n",
        "\n",
        "# using the BeautifulSoup module\n",
        "soup = BeautifulSoup(urlopen(url))\n",
        "\n",
        "# displaying the title\n",
        "print(\"Title of the website is : \")\n",
        "print (soup.title.get_text())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJWKnzII-M01",
        "outputId": "bdbbeb21-06e2-431c-f526-252e0ffba57b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Title of the website is : \n",
            "GeeksforGeeks | A computer science portal for geeks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "from urllib.parse import urlparse\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "response = urllib.request.urlopen('https://www.geeksforgeeks.org/')\n",
        "soup = BeautifulSoup(response,'html.parser',\n",
        "                         from_encoding=response.info().get_param('charset'))\n",
        "#print(soup)\n",
        "if soup.findAll(\"meta\", attrs={\"name\": \"description\"}):\n",
        "    print(\"Meta Data is :\")\n",
        "    print(soup.find(\"meta\", attrs={\"name\": \"description\"}).get(\"content\"))\n",
        "else:\n",
        "    print(\"error\")\n",
        "\n",
        "if soup.findAll(\"title\"):\n",
        "    print(\"Title is: \")\n",
        "    print(soup.find(\"title\").string)\n",
        "else:\n",
        "    print(\"error\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fb0pJ62t-hHP",
        "outputId": "f55fda92-41a0-4160-da7d-d90fc654d0c1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meta Data is :\n",
            "A Computer Science portal for geeks. It contains well written, well thought and well explained computer science and programming articles, quizzes and practice/competitive programming/company interview Questions.\n",
            "Title is: \n",
            "GeeksforGeeks | A computer science portal for geeks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Scrapping Payment Gateways from website**"
      ],
      "metadata": {
        "id": "B73FSlq8-3QC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# List of common payment gateways and their identifiers\n",
        "payment_gateways = {\n",
        "    'PayPal': ['paypal.com', 'www.paypal.com', 'PayPal'],\n",
        "    'Stripe': ['stripe.com', 'www.stripe.com', 'Stripe'],\n",
        "    'Square': ['squareup.com', 'www.squareup.com', 'Square'],\n",
        "    'Authorize.Net': ['authorize.net', 'www.authorize.net', 'Authorize.Net'],\n",
        "    'Amazon Pay': ['pay.amazon.com', 'www.pay.amazon.com', 'Amazon Pay'],\n",
        "    'Adyen': ['adyen.com', 'www.adyen.com', 'Adyen'],\n",
        "    '2Checkout': ['2checkout.com', 'www.2checkout.com', '2Checkout'],\n",
        "    'Braintree': ['braintreepayments.com', 'www.braintreepayments.com', 'Braintree'],\n",
        "    'WorldPay': ['worldpay.com', 'www.worldpay.com', 'WorldPay']\n",
        "}\n",
        "def detect_payment_gateway(url):\n",
        "    try:\n",
        "        # Send a request to the website\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Error accessing {url}: {e}\")\n",
        "        return\n",
        "\n",
        "    # Parse the HTML content\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Check for payment gateway identifiers\n",
        "    found_gateways = set()\n",
        "    for gateway, identifiers in payment_gateways.items():\n",
        "        for identifier in identifiers:\n",
        "            if identifier in response.text:\n",
        "                found_gateways.add(gateway)\n",
        "                  # Print the results\n",
        "    if found_gateways:\n",
        "        print(f\"Payment gateways detected on {url}: {', '.join(found_gateways)}\")\n",
        "    else:\n",
        "        print(f\"No payment gateways detected on {url}\")\n",
        "\n",
        "# Example usage\n",
        "url = input(\"Enter the website URL: \")\n",
        "detect_payment_gateway(url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9ZTBbxu-9mK",
        "outputId": "28298cd1-712c-4acb-f677-019107ea099c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the website URL: https://www.meesho.com/designer-v-smoking-full-sleeves-top/p/5r6sv2\n",
            "Error accessing https://www.meesho.com/designer-v-smoking-full-sleeves-top/p/5r6sv2: 403 Client Error: Forbidden for url: https://www.meesho.com/designer-v-smoking-full-sleeves-top/p/5r6sv2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Scrapping Website Language**"
      ],
      "metadata": {
        "id": "T9XzGxz7AVYd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "request = requests.head('https://en.wikipedia.org/wiki/Main_Page')\n",
        "print(request.headers[\"Content-language\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ae-xWyvq_FwA",
        "outputId": "2fdcaef4-3efa-4fd7-9fa6-47370b599600"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "en\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Finding Category of website**"
      ],
      "metadata": {
        "id": "UnIzVHVdDmrC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from transformers import pipeline\n",
        "\n",
        "def fetch_website_content(url):\n",
        "    \"\"\"Fetches the HTML content of a website.\"\"\"\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        return response.text\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Error fetching the website content: {e}\")\n",
        "        return None\n",
        "\n",
        "def extract_text_from_html(html_content):\n",
        "    \"\"\"Extracts text content from HTML.\"\"\"\n",
        "    soup = BeautifulSoup(html_content, 'html.parser')\n",
        "    text_elements = soup.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
        "    return ' '.join(element.get_text() for element in text_elements if element.get_text().strip())\n",
        "\n",
        "def categorize_website_content(text_content, classifier, candidate_labels):\n",
        "    \"\"\"Classifies text content into categories using a pre-trained classifier.\"\"\"\n",
        "    return classifier(text_content, candidate_labels)\n",
        "\n",
        "def main():\n",
        "    url = input(\"Enter the website URL: \")\n",
        "    html_content = fetch_website_content(url)\n",
        "    if not html_content:\n",
        "        print(\"Could not fetch the website content.\")\n",
        "        return\n",
        "\n",
        "    text_content = extract_text_from_html(html_content)\n",
        "    if not text_content:\n",
        "        print(\"Could not extract text content from the website.\")\n",
        "        return\n",
        "\n",
        "    # Loading  the classifier once\n",
        "    classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
        "    candidate_labels = [\"technology\", \"sports\", \"news\", \"entertainment\", \"education\", \"health\", \"finance\"]\n",
        "\n",
        "    try:\n",
        "        result = categorize_website_content(text_content, classifier, candidate_labels)\n",
        "        print(\"Category:\", result['labels'][0])\n",
        "        print(\"Scores:\", result['scores'])\n",
        "    except Exception as e:\n",
        "        print(f\"Error classifying the website content: {e}\")\n",
        "\n",
        "if _name_ == \"_main_\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "3NHBLNDnDtcU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}