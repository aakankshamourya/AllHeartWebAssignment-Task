# -*- coding: utf-8 -*-
"""AllHeartWeb.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HN1-iieFxfsXtbzCzOhGIjex-qHrcmNu

**Scrapping Social Media Links**
Beautiful Soup is a library that makes it easy to scrape information from web pages. It sits atop an HTML or XML parser, providing Pythonic idioms for iterating, searching, and modifying the parse tree.
"""

import requests
from bs4 import BeautifulSoup
def scrape_social_links(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'html.parser')
    social_links = []
    for link in soup.find_all('a'):
        href = link.get('href')
        if href and any(platform in href for platform in ['facebook', 'twitter', 'linkedin']):
            social_links.append(href)
    return social_links

# Usage
url = "https://allheartweb.com/"
social_links = scrape_social_links(url)
print(f"Scraped social links: {social_links}")



"""# **Scrapping Meta Titles From Website**

"""

# importing the modules
import requests
from bs4 import BeautifulSoup

# target url
url = 'https://www.javatpoint.com/'

# making requests instance
reqs = requests.get(url)

# using the BeautifulSoup module
soup = BeautifulSoup(reqs.text, 'html.parser')

# displaying the title
print("Title of the website is : ")
for title in soup.find_all('title'):
	print(title.get_text())

"""# Meta Description"""

# importing the modules
from urllib.request import urlopen
from bs4 import BeautifulSoup

# target url
url = 'https://www.geeksforgeeks.org/'

# using the BeautifulSoup module
soup = BeautifulSoup(urlopen(url))

# displaying the title
print("Title of the website is : ")
print (soup.title.get_text())

import urllib.request
from urllib.parse import urlparse
from bs4 import BeautifulSoup

response = urllib.request.urlopen('https://www.geeksforgeeks.org/')
soup = BeautifulSoup(response,'html.parser',
                         from_encoding=response.info().get_param('charset'))
#print(soup)
if soup.findAll("meta", attrs={"name": "description"}):
    print("Meta Data is :")
    print(soup.find("meta", attrs={"name": "description"}).get("content"))
else:
    print("error")

if soup.findAll("title"):
    print("Title is: ")
    print(soup.find("title").string)
else:
    print("error")

"""# **Scrapping Payment Gateways from website**"""

import requests
from bs4 import BeautifulSoup

# List of common payment gateways and their identifiers
payment_gateways = {
    'PayPal': ['paypal.com', 'www.paypal.com', 'PayPal'],
    'Stripe': ['stripe.com', 'www.stripe.com', 'Stripe'],
    'Square': ['squareup.com', 'www.squareup.com', 'Square'],
    'Authorize.Net': ['authorize.net', 'www.authorize.net', 'Authorize.Net'],
    'Amazon Pay': ['pay.amazon.com', 'www.pay.amazon.com', 'Amazon Pay'],
    'Adyen': ['adyen.com', 'www.adyen.com', 'Adyen'],
    '2Checkout': ['2checkout.com', 'www.2checkout.com', '2Checkout'],
    'Braintree': ['braintreepayments.com', 'www.braintreepayments.com', 'Braintree'],
    'WorldPay': ['worldpay.com', 'www.worldpay.com', 'WorldPay']
}
def detect_payment_gateway(url):
    try:
        # Send a request to the website
        response = requests.get(url)
        response.raise_for_status()
    except requests.RequestException as e:
        print(f"Error accessing {url}: {e}")
        return

    # Parse the HTML content
    soup = BeautifulSoup(response.text, 'html.parser')

    # Check for payment gateway identifiers
    found_gateways = set()
    for gateway, identifiers in payment_gateways.items():
        for identifier in identifiers:
            if identifier in response.text:
                found_gateways.add(gateway)
                  # Print the results
    if found_gateways:
        print(f"Payment gateways detected on {url}: {', '.join(found_gateways)}")
    else:
        print(f"No payment gateways detected on {url}")

# Example usage
url = input("Enter the website URL: ")
detect_payment_gateway(url)

"""# **Scrapping Website Language**"""

import requests

request = requests.head('https://en.wikipedia.org/wiki/Main_Page')
print(request.headers["Content-language"])

"""# **Finding Category of website**"""

import requests
from bs4 import BeautifulSoup
from transformers import pipeline

def fetch_website_content(url):
    """Fetches the HTML content of a website."""
    try:
        response = requests.get(url)
        response.raise_for_status()
        return response.text
    except requests.RequestException as e:
        print(f"Error fetching the website content: {e}")
        return None

def extract_text_from_html(html_content):
    """Extracts text content from HTML."""
    soup = BeautifulSoup(html_content, 'html.parser')
    text_elements = soup.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6'])
    return ' '.join(element.get_text() for element in text_elements if element.get_text().strip())

def categorize_website_content(text_content, classifier, candidate_labels):
    """Classifies text content into categories using a pre-trained classifier."""
    return classifier(text_content, candidate_labels)

def main():
    url = input("Enter the website URL: ")
    html_content = fetch_website_content(url)
    if not html_content:
        print("Could not fetch the website content.")
        return

    text_content = extract_text_from_html(html_content)
    if not text_content:
        print("Could not extract text content from the website.")
        return

    # Loading  the classifier once
    classifier = pipeline("zero-shot-classification", model="facebook/bart-large-mnli")
    candidate_labels = ["technology", "sports", "news", "entertainment", "education", "health", "finance"]

    try:
        result = categorize_website_content(text_content, classifier, candidate_labels)
        print("Category:", result['labels'][0])
        print("Scores:", result['scores'])
    except Exception as e:
        print(f"Error classifying the website content: {e}")

if _name_ == "_main_":
    main()